# Oficina Microservices

Arquitetura de microserviÃ§os para sistema de gestÃ£o de oficina mecÃ¢nica, deployada em AWS EKS com infraestrutura como cÃ³digo (Terraform) e gerenciamento de mÃºltiplos ambientes via Kustomize.

## ğŸ“‹ Arquitetura

Este projeto utiliza uma arquitetura de microserviÃ§os moderna com as seguintes caracterÃ­sticas:

### VisÃ£o Geral da Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            CAMADA DE ACESSO                              â”‚
â”‚                                                                           â”‚
â”‚  Internet/VPN â†’ AWS API Gateway (HTTP API) â†’ VPC Link (Private)         â”‚
â”‚                           â†“                                               â”‚
â”‚                    AWS Network Load Balancer                             â”‚
â”‚                  (Balanceamento L4 - Porta 8761)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AWS EKS CLUSTER                                  â”‚
â”‚                    (Kubernetes Gerenciado)                               â”‚
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Namespace: oficina-mecanica-{env}                  â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
â”‚  â”‚  â”‚ Eureka Server  â”‚ â†â”€â”€â”€â”€â”€â”€â†’ â”‚  Metrics Server     â”‚           â”‚   â”‚
â”‚  â”‚  â”‚  (Port 8761)   â”‚          â”‚  (HPA Support)      â”‚           â”‚   â”‚
â”‚  â”‚  â”‚                â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Service      â”‚                                             â”‚   â”‚
â”‚  â”‚  â”‚   Discovery    â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
â”‚  â”‚  â”‚ â€¢ Swagger      â”‚          â”‚   ConfigMaps &      â”‚           â”‚   â”‚
â”‚  â”‚  â”‚   Agregado     â”‚ â†â”€â”€â”€â”€â”€â”€â†’ â”‚   Secrets           â”‚           â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  (Terraform)        â”‚           â”‚   â”‚
â”‚  â”‚           â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
â”‚  â”‚           â”‚ Service Discovery via Feign Clients                â”‚   â”‚
â”‚  â”‚           â†“                                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚              MICROSERVIÃ‡OS BACKEND                      â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                                                          â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚   Auth   â”‚  â”‚ Customer â”‚  â”‚ Catalog  â”‚             â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  (8082)  â”‚  â”‚  (8081)  â”‚  â”‚  (8083)  â”‚             â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜             â”‚   â”‚   â”‚
â”‚  â”‚  â”‚        â”‚              â”‚              â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”            â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ Inventory  â”‚ â”‚ Budget  â”‚  â”‚   Work   â”‚            â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â”‚   (8084)   â”‚ â”‚ (8085)  â”‚  â”‚  Order   â”‚            â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  (8086)  â”‚            â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                               â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜             â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                                     â”‚                  â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                              â”‚Notification â”‚          â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                              â”‚   (8087)    â”‚          â”‚   â”‚   â”‚
â”‚  â”‚  â”‚                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                                        â”‚                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚         HORIZONTAL POD AUTOSCALER (HPA)                â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Min: 1-2 replicas (dev/prod)                        â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Max: 2-5 replicas (dev/prod)                        â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Target CPU: 70%                                     â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CAMADA DE DADOS                                   â”‚
â”‚                                                                           â”‚
â”‚  â€¢ AWS RDS PostgreSQL (Dev/Prod) ou                                     â”‚
â”‚  â€¢ PostgreSQL StatefulSet (Local)                                       â”‚
â”‚  â€¢ AWS Secrets Manager (Prod - recomendado)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ï¿½ğŸ—ï¸ Infraestrutura AWS

A infraestrutura Ã© provisionada automaticamente via **Terraform** e inclui:

#### Kubernetes (EKS)
- **EKS Cluster**: Cluster Kubernetes gerenciado pela AWS (assumido como prÃ©-existente: `eks-fiap-oficina-mecanica`)
- **Namespaces isolados**: `oficina-mecanica-dev` e `oficina-mecanica-prod`
- **Metrics Server**: Auto-provisionado para suporte ao HPA (Horizontal Pod Autoscaler)
- **AWS NLB Controller**: Gerencia Network Load Balancers para expor serviÃ§os

#### Rede e Acesso
- **Network Load Balancer (NLB)**: Balanceador L4 por ambiente (dev/prod) expondo o Eureka Server na porta 8761
- **VPC Link**: Conecta o API Gateway ao NLB interno para acesso seguro aos microserviÃ§os
- **Security Groups**: Isolamento de rede entre componentes (VPC Link, NLB, EKS)
- **API Gateway (HTTP API)**: Ponto de entrada Ãºnico via AWS API Gateway V2 (integraÃ§Ã£o com NLB via VPC Link)

#### Dados e ConfiguraÃ§Ã£o
- **RDS PostgreSQL** (assumido): Banco de dados gerenciado para ambientes dev/prod
- **ConfigMaps**: ConfiguraÃ§Ãµes compartilhadas injetadas dinamicamente via Terraform
- **Secrets**: Gerenciados via Kubernetes Secrets (dev) e recomendado AWS Secrets Manager (prod)
- **Datadog Integration**: Observabilidade com API key e chaves de aplicaÃ§Ã£o configurÃ¡veis

#### Deployment Automatizado
- **Kustomize**: OrquestraÃ§Ã£o de manifestos Kubernetes com overlays por ambiente
- **Terraform Workspaces**: `dev` e `prod` isolados, cada um com seu prÃ³prio estado
- **Script de atualizaÃ§Ã£o**: `update_kustomize.sh` injeta tags de imagem dinamicamente durante o deploy

### ğŸ¯ OrganizaÃ§Ã£o dos MicroserviÃ§os

#### Service Discovery - Netflix Eureka

Todos os microserviÃ§os se registram automaticamente no **Eureka Server**, permitindo comunicaÃ§Ã£o dinÃ¢mica entre serviÃ§os sem URLs hardcoded. O Eureka tambÃ©m expÃµe um Swagger UI agregado que consolida a documentaÃ§Ã£o de todos os microserviÃ§os.

**Dashboard do Eureka:** http://localhost:8761 (local) ou via API Gateway (cloud)

#### ServiÃ§os Backend

- **eureka-server** (porta 8761) - Service Registry e Swagger agregado
- **auth-service** (porta 8082) - AutenticaÃ§Ã£o JWT e gerenciamento de usuÃ¡rios
- **customer-service** (porta 8081) - GestÃ£o de clientes e veÃ­culos
- **catalog-service** (porta 8083) - CatÃ¡logo de serviÃ§os e produtos
- **inventory-service** (porta 8084) - Controle de estoque
- **budget-service** (porta 8085) - GestÃ£o de orÃ§amentos
- **work-order-service** (porta 8086) - Ordens de serviÃ§o e rastreamento
- **notification-service** (porta 8087) - NotificaÃ§Ãµes assÃ­ncronas por email

#### Arquitetura de ComunicaÃ§Ã£o

```
Internet â†’ API Gateway â†’ VPC Link â†’ NLB (8761) â†’ Eureka Server (K8s)
                                              â†“
                                         Service Mesh
                                              â†“
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â†“                       â†“                       â†“
                 Auth Service          Customer Service       Catalog Service
                  (Port 8082)            (Port 8081)            (Port 8083)
                      â†“                       â†“                       â†“
                 Inventory Service      Budget Service      Work Order Service
                  (Port 8084)            (Port 8085)            (Port 8086)
                                              â†“
                                    Notification Service
                                        (Port 8087)
                                              â†“
                                      PostgreSQL (RDS/Pod)
```

Todos os serviÃ§os se comunicam via **Feign Clients** usando service discovery (nomes lÃ³gicos como `auth-service`, `customer-service`), que o Eureka resolve para IPs dinÃ¢micos dos pods.

### ğŸ”§ Kustomize: Gerenciamento Multi-Ambiente

O projeto usa **Kustomize overlays** para suportar 3 ambientes distintos:

#### Estrutura

```
k8s/
â”œâ”€â”€ base/                    # Manifestos comuns (Deployments, Services, HPA)
â”‚   â”œâ”€â”€ postgres.yaml        # StatefulSet PostgreSQL (apenas local)
â”‚   â”œâ”€â”€ eureka-server.yaml   # Deployment + Service + NLB annotations
â”‚   â”œâ”€â”€ *-service.yaml       # 7 microserviÃ§os (Deployment + Service)
â”‚   â”œâ”€â”€ configmap-shared.yaml
â”‚   â””â”€â”€ hpa.yaml             # Horizontal Pod Autoscaler
â”‚
â””â”€â”€ overlays/
    â”œâ”€â”€ local/               # Minikube (namespace: oficina)
    â”‚   â””â”€â”€ Secrets hardcoded, Ingress local, PostgreSQL incluso
    â”‚
    â”œâ”€â”€ dev/                 # EKS Dev (namespace: oficina-mecanica-dev)
    â”‚   â””â”€â”€ NLB interno, sem PostgreSQL (usa RDS), ConfigMap via Terraform
    â”‚
    â””â”€â”€ prod/                # EKS Prod (namespace: oficina-mecanica-prod)
        â””â”€â”€ 2+ rÃ©plicas, mais recursos, NLB pÃºblico, secrets externos
```

#### DiferenÃ§as Entre Ambientes

| CaracterÃ­stica       | Local (Minikube)       | Dev (EKS)                 | Prod (EKS)                |
|---------------------|------------------------|---------------------------|---------------------------|
| **Namespace**       | `oficina`              | `oficina-mecanica-dev`    | `oficina-mecanica-prod`   |
| **PostgreSQL**      | Pod StatefulSet        | RDS (assumido)            | RDS (assumido)            |
| **NLB Scheme**      | N/A (Ingress local)    | `internal`                | `internet-facing`         |
| **RÃ©plicas Base**   | 1                      | 1                         | 2                         |
| **HPA Min/Max**     | 1-2                    | 1-2                       | 2-5                       |
| **ConfigMap**       | Arquivo YAML           | Injetado via Terraform    | Injetado via Terraform    |
| **Secrets**         | Hardcoded (OK p/ dev)  | Kubernetes Secrets        | AWS Secrets Manager       |
| **Gerenciamento**   | `kubectl apply`        | Terraform (automÃ¡tico)    | Terraform (automÃ¡tico)    |

#### Como o Kustomize Ã© Aplicado

**Local (Manual):**
```bash
kubectl apply -k k8s/overlays/local
```

**Dev/Prod (Terraform Automatizado):**
1. Terraform executa `scripts/update_kustomize.sh` com a tag da imagem desejada
2. Script atualiza `kustomization.yaml` com as novas tags usando `kustomize edit set image`
3. Script executa `kubectl kustomize` e retorna os manifestos finais em base64
4. Terraform aplica os manifestos via `kubectl_manifest` com **server-side apply** (evita conflitos com HPA)

### ğŸŒ API Gateway: Roteamento Centralizado

#### Fluxo de RequisiÃ§Ãµes

```
Cliente â†’ API Gateway (AWS) â†’ VPC Link â†’ NLB â†’ Eureka Service (K8s) â†’ Backend Services
```

#### Mapeamento de Rotas (Planejado)

| Prefixo        | ServiÃ§o Backend       | Porta | DescriÃ§Ã£o                     |
|----------------|----------------------|-------|-------------------------------|
| `/eureka`      | eureka-server        | 8761  | Service Discovery & Swagger   |
| `/auth`        | auth-service         | 8082  | AutenticaÃ§Ã£o JWT              |
| `/customer`    | customer-service     | 8081  | Clientes & VeÃ­culos           |
| `/catalog`     | catalog-service      | 8083  | CatÃ¡logo de Produtos          |
| `/inventory`   | inventory-service    | 8084  | Controle de Estoque           |
| `/budget`      | budget-service       | 8085  | OrÃ§amentos                    |
| `/work-order`  | work-order-service   | 8086  | Ordens de ServiÃ§o             |
| `/notification`| notification-service | 8087  | NotificaÃ§Ãµes                  |

**Exemplo de URL:**
```
https://api.oficina-mecanica.com/auth/api/auth/login
```

#### Componentes do Roteamento

1. **AWS API Gateway (HTTP API)**: Ponto de entrada pÃºblico/privado
2. **VPC Link**: Conecta API Gateway Ã  rede privada do EKS
3. **Network Load Balancer**: Balanceador L4 expondo porta 8761 (Eureka)
4. **Kubernetes Service (NLB)**: Service type `LoadBalancer` com annotations AWS
5. **Eureka Server**: Roteia internamente para os microserviÃ§os via service discovery

---

## ğŸ  Desenvolvimento Local - Guia Completo

Este guia detalha **3 formas de rodar o projeto localmente** para desenvolvimento. Escolha a que melhor se adequa ao seu cenÃ¡rio.

### ğŸ“‹ PrÃ©-requisitos

Antes de comeÃ§ar, certifique-se de ter instalado:

- âœ… **Java 21** - [Download](https://adoptium.net/)
- âœ… **Maven 3.9+** - [Download](https://maven.apache.org/download.cgi)
- âœ… **Docker & Docker Compose** - [Download](https://www.docker.com/products/docker-desktop)
- âœ… **Git** - Para clonar o repositÃ³rio

**Verificar instalaÃ§Ã£o:**
```bash
java -version    # Deve mostrar Java 21
mvn -version     # Deve mostrar Maven 3.9+
docker --version # Deve mostrar Docker 20.10+
docker compose version
```

---

### ğŸ¯ OpÃ§Ã£o 1: Docker Compose (â­ Recomendado)

**Vantagens:** RÃ¡pido, isolado, nÃ£o precisa configurar banco manualmente, simula ambiente de produÃ§Ã£o.

#### Passo a Passo

**1. Clonar o repositÃ³rio**
```bash
git clone https://github.com/seu-usuario/oficina-microservices.git
cd oficina-microservices
```

**2. Subir todos os serviÃ§os**
```bash
# Sobe PostgreSQL + Eureka + todos os 7 microserviÃ§os
docker compose --profile dev up -d

# Acompanhar os logs (Ctrl+C para sair)
docker compose --profile dev logs -f
```

**3. Aguardar inicializaÃ§Ã£o (â±ï¸ ~2-3 minutos)**

O Docker Compose inicia os serviÃ§os na ordem correta:
1. PostgreSQL (porta 5432)
2. Eureka Server (porta 8761)
3. Auth Service (porta 8082)
4. Demais microserviÃ§os (portas 8081-8087)

**4. Verificar se tudo estÃ¡ funcionando**

```bash
# Ver status de todos os containers
docker compose --profile dev ps

# Todos devem estar "healthy" ou "running"
# Se algum estiver "unhealthy", veja os logs:
docker compose --profile dev logs auth-service
```

**5. Acessar os serviÃ§os**

ğŸŒ **Eureka Dashboard (Service Registry):**
- URL: http://localhost:8761
- Aguarde atÃ© ver todos os 7 serviÃ§os registrados

ğŸ“– **Swagger Agregado (Todas as APIs em um lugar):**
- URL: http://localhost:8761/swagger-ui.html
- Use o dropdown para selecionar cada serviÃ§o

**6. Testar uma chamada (exemplo)**

```bash
# 1. Criar um usuÃ¡rio
curl -X POST http://localhost:8082/api/auth/register \
  -H "Content-Type: application/json" \
  -d '{
    "nome": "JoÃ£o Silva",
    "email": "joao@example.com",
    "senha": "senha123",
    "role": "MECANICO"
  }'

# 2. Fazer login
curl -X POST http://localhost:8082/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{
    "email": "joao@example.com",
    "senha": "senha123"
  }'

# Copie o token JWT retornado para usar nas prÃ³ximas chamadas
```

**7. Parar tudo quando terminar**

```bash
# Parar mas manter os dados
docker compose --profile dev stop

# Parar e remover containers (mantÃ©m volumes/dados)
docker compose --profile dev down

# Parar e LIMPAR TUDO (incluindo banco de dados)
docker compose --profile dev down -v
```

#### ğŸ”§ Comandos Ãšteis - Docker Compose

```bash
# Ver logs de um serviÃ§o especÃ­fico
docker compose --profile dev logs -f customer-service

# Reiniciar um serviÃ§o especÃ­fico
docker compose --profile dev restart auth-service

# Rebuild apÃ³s mudanÃ§as no cÃ³digo
docker compose --profile dev up -d --build

# Ver uso de recursos
docker stats

# Acessar terminal de um container
docker exec -it customer-service bash
```

---

### ğŸ¯ OpÃ§Ã£o 2: Minikube (Kubernetes Local)

**Vantagens:** Testa deploy em Kubernetes, mais prÃ³ximo do ambiente de produÃ§Ã£o.

#### Passo a Passo

**1. Instalar Minikube**
```bash
# macOS
brew install minikube

# Linux
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube

# Verificar
minikube version
```

**2. Iniciar Minikube**
```bash
# Subir cluster local com recursos adequados
minikube start --cpus=4 --memory=8192 --driver=docker

# Habilitar addons necessÃ¡rios
minikube addons enable ingress
minikube addons enable metrics-server
```

**3. Build das imagens localmente (usar Docker do Minikube)**

```bash
# Configurar shell para usar Docker do Minikube
eval $(minikube docker-env)

# Build de todas as imagens
cd oficina-microservices
make build TAG=latest

# Ou build manual
mvn clean install -DskipTests
docker build -t grecomilani/oficina-eureka-server:latest -f eureka-server/Dockerfile .
docker build -t grecomilani/oficina-auth-service:latest -f auth-service/Dockerfile .
# ... repetir para todos os serviÃ§os
```

**4. Aplicar manifestos Kubernetes**

```bash
# Usar o overlay local (namespace: oficina)
kubectl apply -k k8s/overlays/local

# Acompanhar os pods subindo
kubectl -n oficina get pods -w
```

**5. Aguardar todos os pods ficarem Running (â±ï¸ ~3-5 minutos)**

```bash
# Verificar status
kubectl -n oficina get pods

# Todos devem estar 1/1 Running
# Se algum estiver CrashLoopBackOff:
kubectl -n oficina logs -f pod/<nome-do-pod>
```

**6. Acessar os serviÃ§os**

**OpÃ§Ã£o A: Port-Forward (Recomendado para dev)**
```bash
# Eureka Dashboard
kubectl -n oficina port-forward svc/eureka-server 8761:8761

# Auth Service
kubectl -n oficina port-forward svc/auth-service 8082:8082

# Customer Service
kubectl -n oficina port-forward svc/customer-service 8081:8081

# Acesse: http://localhost:8761
```

**OpÃ§Ã£o B: Ingress (acesso via domÃ­nio)**
```bash
# Adicionar ao /etc/hosts
echo "$(minikube ip) oficina.local" | sudo tee -a /etc/hosts

# Acessar via:
# http://oficina.local/eureka
# http://oficina.local/auth
# http://oficina.local/customer
```

**7. Limpar tudo**

```bash
# Deletar todos os recursos
kubectl delete -k k8s/overlays/local

# Parar Minikube
minikube stop

# Deletar cluster completamente
minikube delete
```

---

### ğŸ¯ OpÃ§Ã£o 3: Maven Local (Sem Containers)

**Vantagens:** Ãštil para debug, desenvolvimento isolado de um serviÃ§o, nÃ£o precisa de Docker.

**âš ï¸ AtenÃ§Ã£o:** VocÃª precisarÃ¡ de um PostgreSQL rodando (pode usar Docker apenas para o banco).

#### Passo a Passo

**1. Subir PostgreSQL (via Docker)**
```bash
docker run -d \
  --name postgres-oficina \
  -e POSTGRES_DB=oficina-db \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -p 5432:5432 \
  postgres:15-alpine

# Verificar se estÃ¡ rodando
docker ps | grep postgres-oficina
```

**2. Build do projeto**
```bash
cd oficina-microservices

# Compilar todos os mÃ³dulos (necessÃ¡rio por causa da shared-library)
mvn clean install -DskipTests
```

**3. Iniciar Eureka Server (SEMPRE primeiro!)**
```bash
cd eureka-server
mvn spring-boot:run

# Aguarde ver a mensagem:
# "Started EurekaServerApplication in X seconds"

# Acesse: http://localhost:8761
```

**4. Iniciar serviÃ§os (cada um em um terminal separado)**

```bash
# Terminal 2 - Auth Service
cd auth-service
mvn spring-boot:run

# Terminal 3 - Customer Service
cd customer-service
mvn spring-boot:run

# Terminal 4 - Catalog Service
cd catalog-service
mvn spring-boot:run

# Terminal 5 - Inventory Service
cd inventory-service
mvn spring-boot:run

# Terminal 6 - Budget Service
cd budget-service
mvn spring-boot:run

# Terminal 7 - Work Order Service
cd work-order-service
mvn spring-boot:run

# Terminal 8 - Notification Service
cd notification-service
mvn spring-boot:run
```

**Dica:** Use **tmux** ou **screen** para gerenciar mÃºltiplos terminais:
```bash
# Instalar tmux
brew install tmux  # macOS
sudo apt install tmux  # Linux

# Criar sessÃ£o tmux
tmux new -s oficina

# Dividir em painÃ©is: Ctrl+b entÃ£o "
# Navegar entre painÃ©is: Ctrl+b entÃ£o seta
```

**5. Verificar serviÃ§os no Eureka**

Acesse http://localhost:8761 e confirme que todos os 7 serviÃ§os aparecem na lista "Instances currently registered with Eureka".

**6. Parar tudo**

```bash
# Parar cada terminal com Ctrl+C

# Parar PostgreSQL
docker stop postgres-oficina
docker rm postgres-oficina
```

---

## âœ… Checklist de VerificaÃ§Ã£o

ApÃ³s subir o ambiente (qualquer opÃ§Ã£o), verifique:

- [ ] âœ… **Eureka Dashboard** (http://localhost:8761) mostra 7-8 serviÃ§os registrados
- [ ] âœ… **Swagger Agregado** (http://localhost:8761/swagger-ui.html) abre corretamente
- [ ] âœ… **Health checks** funcionando:
  ```bash
  curl http://localhost:8761/actuator/health  # Eureka
  curl http://localhost:8082/actuator/health  # Auth
  curl http://localhost:8081/actuator/health  # Customer
  ```
- [ ] âœ… **PostgreSQL** estÃ¡ acessÃ­vel (porta 5432)
- [ ] âœ… **Sem erros** nos logs dos serviÃ§os

---

## ğŸŒ Endpoints Importantes (Local)

| ServiÃ§o | Porta | Swagger | Actuator Health |
|---------|-------|---------|-----------------|
| **Eureka Server** | 8761 | http://localhost:8761/swagger-ui.html | http://localhost:8761/actuator/health |
| **Auth Service** | 8082 | http://localhost:8082/swagger-ui.html | http://localhost:8082/actuator/health |
| **Customer Service** | 8081 | http://localhost:8081/swagger-ui.html | http://localhost:8081/actuator/health |
| **Catalog Service** | 8083 | http://localhost:8083/swagger-ui.html | http://localhost:8083/actuator/health |
| **Inventory Service** | 8084 | http://localhost:8084/swagger-ui.html | http://localhost:8084/actuator/health |
| **Budget Service** | 8085 | http://localhost:8085/swagger-ui.html | http://localhost:8085/actuator/health |
| **Work Order Service** | 8086 | http://localhost:8086/swagger-ui.html | http://localhost:8086/actuator/health |
| **Notification Service** | 8087 | - | http://localhost:8087/actuator/health |

---

## ğŸ› Troubleshooting - Problemas Comuns em Local

### ServiÃ§o nÃ£o registra no Eureka

**Sintomas:** ServiÃ§o sobe mas nÃ£o aparece em http://localhost:8761

**SoluÃ§Ãµes:**
```bash
# 1. Verifique se o Eureka estÃ¡ rodando
curl http://localhost:8761/actuator/health

# 2. Aguarde 30 segundos (delay normal de registro)

# 3. Veja os logs do serviÃ§o
docker compose --profile dev logs auth-service | grep -i eureka

# 4. Verifique se a porta do Eureka estÃ¡ correta
# Deve estar acessÃ­vel em localhost:8761
```

### Porta jÃ¡ em uso

**Sintomas:** Erro "Address already in use" ou "bind: address already in use"

**SoluÃ§Ãµes:**
```bash
# Ver o que estÃ¡ usando a porta
lsof -i :8761  # Substitua pelo nÃºmero da porta
netstat -tulpn | grep 8761

# Matar o processo
kill -9 <PID>

# Ou mudar a porta no application.yml do serviÃ§o
```

### Banco de dados nÃ£o conecta

**Sintomas:** Erros de "Connection refused" ou "Could not connect to database"

**SoluÃ§Ãµes:**
```bash
# 1. Verificar se PostgreSQL estÃ¡ rodando
docker ps | grep postgres

# 2. Testar conexÃ£o diretamente
docker exec -it postgres psql -U postgres -d oficina-db

# 3. Verificar configuraÃ§Ã£o no application.yml
# URL deve ser: jdbc:postgresql://localhost:5432/oficina-db
# Para Docker Compose: jdbc:postgresql://postgres:5432/oficina-db
```

### ServiÃ§os nÃ£o se comunicam (Feign errors)

**Sintomas:** Erros 404 ou "Load balancer does not have available server"

**SoluÃ§Ãµes:**
```bash
# 1. Todos os serviÃ§os devem estar no Eureka
curl http://localhost:8761/eureka/apps

# 2. Aguarde 30-60 segundos apÃ³s todos subirem

# 3. Verifique se os nomes dos serviÃ§os estÃ£o corretos
# Devem ser exatamente: auth-service, customer-service, etc.

# 4. Veja logs de Feign no serviÃ§o
docker compose --profile dev logs work-order-service | grep -i feign
```

### Build do Maven falha

**Sintomas:** Erros de compilaÃ§Ã£o, testes falhando, dependÃªncias nÃ£o encontradas

**SoluÃ§Ãµes:**
```bash
# 1. Limpar cache do Maven
mvn clean

# 2. Rebuild completo (do diretÃ³rio raiz!)
cd oficina-microservices
mvn clean install -DskipTests

# 3. Se shared-library nÃ£o for encontrada
cd shared-library
mvn clean install
cd ..

# 4. Limpar cache local se necessÃ¡rio
rm -rf ~/.m2/repository/br/com/fiap/oficina
mvn clean install
```

### Containers ficam "unhealthy"

**Sintomas:** `docker compose ps` mostra status "unhealthy"

**SoluÃ§Ãµes:**
```bash
# 1. Ver logs do container
docker compose --profile dev logs <service-name>

# 2. Verificar health check manualmente
docker exec <container-name> wget -qO- http://localhost:8082/actuator/health

# 3. Aumentar tempo de inicializaÃ§Ã£o
# Edite docker-compose.yml e aumente start_period no healthcheck

# 4. Rebuild a imagem
docker compose --profile dev up -d --build <service-name>
```

---

## ğŸ“ PrÃ³ximos Passos ApÃ³s Rodar Local

1. ğŸ“– **Explore a API** via Swagger: http://localhost:8761/swagger-ui.html
2. ğŸ§ª **Rode os testes**: `mvn test`
3. ğŸ“Š **Monitore no Eureka**: http://localhost:8761
4. ğŸ” **Veja o CLAUDE.md** na raiz para entender a arquitetura completa
5. ğŸš€ **Deploy em Dev/Prod**: Veja `k8s/README-OVERLAYS.md`

---

---

## ğŸš€ Deploy em ProduÃ§Ã£o (AWS EKS)

### PrÃ©-requisitos AWS

- AWS CLI configurado (`aws configure`)
- Terraform >= 1.5
- EKS Cluster prÃ©-existente (`eks-fiap-oficina-mecanica`)
- RDS PostgreSQL configurado (ou usar PostgreSQL no Kubernetes)
- Imagens Docker publicadas no Docker Hub

### Deploy via Terraform

```bash
cd infra

# 1. Inicializar Terraform (primeira vez)
terraform init

# 2. Selecionar workspace (dev ou prod)
terraform workspace select dev
# ou
terraform workspace new prod
terraform workspace select prod

# 3. Planejar mudanÃ§as
terraform plan -var-file=environments/dev.tfvars

# 4. Aplicar infraestrutura
terraform apply -var-file=environments/dev.tfvars

# O Terraform irÃ¡:
# âœ“ Criar namespace (oficina-mecanica-dev ou oficina-mecanica-prod)
# âœ“ Criar ConfigMaps com URLs de banco e Eureka
# âœ“ Criar Secrets (JWT, database, notification)
# âœ“ Provisionar NLB para expor Eureka (porta 8761)
# âœ“ Criar VPC Link para integraÃ§Ã£o com API Gateway
# âœ“ Aplicar Kustomize overlays (todos os deployments e services)
# âœ“ Instalar Metrics Server (apenas dev)
# âœ“ Configurar AWS NLB Controller

# 5. Verificar recursos criados
kubectl config use-context <contexto-do-eks>
kubectl -n oficina-mecanica-dev get pods
kubectl -n oficina-mecanica-dev get svc

# 6. Obter URL do Load Balancer
terraform output nlb_dns_name
# ou
kubectl -n oficina-mecanica-dev get svc eureka-server-nlb
```

### Atualizar VersÃ£o das Imagens

```bash
cd infra

# Aplicar com nova tag
terraform apply -var="image_tag=v2.0.0" -var-file=environments/dev.tfvars

# O script update_kustomize.sh serÃ¡ executado automaticamente
# e atualizarÃ¡ todas as imagens para a nova tag
```

### Destruir Infraestrutura

```bash
cd infra
terraform workspace select dev
terraform destroy -var-file=environments/dev.tfvars
```

### ğŸ”‘ Comandos Terraform Ãšteis

```bash
# Listar workspaces
terraform workspace list

# Ver estado atual
terraform show

# Ver outputs (URLs, ARNs, etc.)
terraform output
terraform output nlb_dns_name
terraform output vpc_link_id

# Validar configuraÃ§Ã£o
terraform validate

# Formatar cÃ³digo
terraform fmt -recursive

# Ver plano detalhado
terraform plan -var-file=environments/dev.tfvars -out=tfplan
terraform show tfplan

# Aplicar plano salvo
terraform apply tfplan

# Refresh state (sincronizar com recursos reais)
terraform refresh -var-file=environments/dev.tfvars

# Listar recursos no state
terraform state list

# Ver detalhes de um recurso
terraform state show kubectl_manifest.kustomization[\"v1/Namespace/oficina-mecanica-dev\"]

# Remover recurso do state (sem deletar o recurso real)
terraform state rm kubernetes_namespace.oficina

# Importar recurso existente
terraform import kubernetes_namespace.oficina oficina-mecanica-dev

# Unlock state se travado
terraform force-unlock <LOCK_ID>
```

### ğŸ”§ Comandos Kustomize Ãšteis

```bash
# Ver manifests finais sem aplicar (dry-run)
kubectl kustomize k8s/overlays/dev
kubectl kustomize k8s/overlays/prod > preview.yaml

# Validar sintaxe
kubectl kustomize k8s/overlays/dev --enable-helm

# Comparar diferenÃ§as entre ambientes
diff <(kubectl kustomize k8s/overlays/dev) <(kubectl kustomize k8s/overlays/prod)

# Atualizar imagens manualmente
cd k8s/overlays/dev
kustomize edit set image \
  grecomilani/oficina-eureka-server=grecomilani/oficina-eureka-server:v2.0.0 \
  grecomilani/oficina-auth-service=grecomilani/oficina-auth-service:v2.0.0
  
# Verificar versÃ£o atual das imagens
grep -r "newName:" k8s/overlays/dev/kustomization.yaml
```

### ğŸš€ Comandos Kubernetes Ãšteis

```bash
# Contexto e namespace
kubectl config current-context
kubectl config use-context <context-name>
kubectl config set-context --current --namespace=oficina-mecanica-dev

# Pods e logs
kubectl -n oficina-mecanica-dev get pods -o wide
kubectl -n oficina-mecanica-dev logs -f <pod-name>
kubectl -n oficina-mecanica-dev logs -f deployment/auth-service
kubectl -n oficina-mecanica-dev logs -f <pod-name> --previous  # logs do pod anterior (crash)

# Executar comandos em pods
kubectl -n oficina-mecanica-dev exec -it <pod-name> -- /bin/sh
kubectl -n oficina-mecanica-dev exec -it <pod-name> -- curl http://localhost:8082/actuator/health

# Deployments
kubectl -n oficina-mecanica-dev get deployments
kubectl -n oficina-mecanica-dev describe deployment auth-service
kubectl -n oficina-mecanica-dev rollout status deployment/auth-service
kubectl -n oficina-mecanica-dev rollout restart deployment/auth-service
kubectl -n oficina-mecanica-dev rollout undo deployment/auth-service  # rollback

# Services e endpoints
kubectl -n oficina-mecanica-dev get svc
kubectl -n oficina-mecanica-dev get endpoints
kubectl -n oficina-mecanica-dev describe svc eureka-server-nlb

# HPA (Horizontal Pod Autoscaler)
kubectl -n oficina-mecanica-dev get hpa
kubectl -n oficina-mecanica-dev describe hpa auth-service-hpa
kubectl -n oficina-mecanica-dev top pods  # uso de CPU/memÃ³ria

# ConfigMaps e Secrets
kubectl -n oficina-mecanica-dev get configmaps
kubectl -n oficina-mecanica-dev describe configmap oficina-shared-config
kubectl -n oficina-mecanica-dev get secrets
kubectl -n oficina-mecanica-dev get secret auth-jwt-secret -o jsonpath='{.data.JWT_SECRET}' | base64 -d

# Events (debug de problemas)
kubectl -n oficina-mecanica-dev get events --sort-by='.lastTimestamp'
kubectl -n oficina-mecanica-dev get events --field-selector type=Warning

# Describe para troubleshooting
kubectl -n oficina-mecanica-dev describe pod <pod-name>
kubectl -n oficina-mecanica-dev describe node <node-name>

# Delete recursos
kubectl -n oficina-mecanica-dev delete pod <pod-name>  # recria automaticamente
kubectl -n oficina-mecanica-dev delete deployment auth-service
kubectl delete -k k8s/overlays/dev  # deleta todo o overlay

# Port-forward para debug
kubectl -n oficina-mecanica-dev port-forward svc/eureka-server 8761:8761
kubectl -n oficina-mecanica-dev port-forward deployment/auth-service 8082:8082
```

---

## ğŸš€ Outras Formas de Executar

### PrÃ©-requisitos

- Java 21
- Maven 3.9+
- Docker e Docker Compose

### Ordem de InicializaÃ§Ã£o

Ã‰ importante iniciar os serviÃ§os na ordem correta:

1. **Eureka Server** (primeiro)
2. **Demais microserviÃ§os**

### Executar com Docker Compose

```bash
# Dev: build local e sobe todos os serviÃ§os
docker compose --profile dev up -d

# Prod: usa imagens publicadas (defina REGISTRY/TAG)
REGISTRY=seu-usuario TAG=latest docker compose --profile prod -f docker-compose.yml -f docker-compose.prod.yml up -d

# Verificar o status dos serviÃ§os
docker compose --profile dev ps

# Visualizar logs
docker compose --profile dev logs -f

# Parar todos os serviÃ§os
docker compose --profile dev down
docker compose --profile prod -f docker-compose.yml -f docker-compose.prod.yml down
```

### Executar Localmente (para desenvolvimento)

```bash
# 1. Compilar todos os mÃ³dulos
mvn clean install -DskipTests

# 2. Iniciar o Eureka Server primeiro
cd eureka-server
mvn spring-boot:run

# 3. Em outros terminais, iniciar os demais serviÃ§os
cd auth-service
mvn spring-boot:run

cd customer-service
mvn spring-boot:run

# ... e assim por diante
```

### Publicar imagens no Docker Hub com Makefile

Use o `Makefile` da raiz para padronizar build/push das imagens (cada dev usa seu prÃ³prio Docker Hub):

```bash
# 1) FaÃ§a login no seu Docker Hub
docker login

# 2) Build de todas as imagens (tag padrÃ£o: latest)
make build TAG=latest

# 3) Build + push para seu namespace (troque pelo seu usuÃ¡rio)
make push REGISTRY=seu-usuario TAG=latest

# 4) Multi-arch (amd64+arm64) com buildx e push
make buildx-push REGISTRY=seu-usuario TAG=latest PLATFORMS='linux/amd64,linux/arm64'

# 5) Limpar imagens locais geradas
make clean TAG=latest
```

Dicas rÃ¡pidas:
- `REGISTRY` Ã© obrigatÃ³rio para push (ex.: `REGISTRY=grecomilani`).
- Ajuste `TAG` conforme a versÃ£o que quiser publicar (ex.: `v1.0.0`).
- Para multi-arch, garanta um builder ativo: `docker buildx create --name multi --use` (uma vez sÃ³).
- Se usar tags publicadas no Kubernetes, atualize os `image:` em `k8s/*.yaml` para `seu-usuario/oficina-<serviÃ§o>:<tag>`.

### Rodar no Minikube (k8s/)

```bash
# 1) Subir o Minikube com recursos mÃ­nimos
minikube start --cpus=4 --memory=8192 --kubernetes-version=v1.26.0

# Add-ons Ãºteis (antes do apply, uma vez sÃ³)
minikube addons enable metrics-server
minikube addons enable dashboard
# Se quiser ingress (roteamento HTTP Ãºnico):
minikube addons enable ingress
# Adicione ao /etc/hosts: \"$(minikube ip) oficina.local\"
# Ingress disponÃ­vel em http://oficina.local/<path> (ver caminhos abaixo)
# Abrir dashboard (faz proxy e abre no navegador)
minikube dashboard

# 2) Disponibilizar imagens no cluster:
#    2a) Se jÃ¡ publicou no Docker Hub (ex.: grecomilani), pule para o passo 3
#    2b) Ou carregue no daemon do Minikube:
eval "$(minikube docker-env)"
make build TAG=latest
for img in grecomilani/oficina-{eureka-server,auth-service,customer-service,catalog-service,inventory-service,budget-service,work-order-service,notification-service}:latest; do
  minikube image load "$img"
done

# 3) Aplicar manifests (kustomize jÃ¡ cuida da ordem)
kubectl apply -k k8s

# 4) Acompanhar os pods
kubectl -n oficina get pods -w

# 5) Port-forward para acessar (um terminal por serviÃ§o ou use tmux)
kubectl -n oficina port-forward svc/eureka-server 8761:8761           # Eureka / Swagger agregado
kubectl -n oficina port-forward svc/auth-service 8082:8082            # Auth
kubectl -n oficina port-forward svc/customer-service 8081:8081        # Customer
kubectl -n oficina port-forward svc/catalog-service 8083:8083         # Catalog
kubectl -n oficina port-forward svc/inventory-service 8084:8084       # Inventory
kubectl -n oficina port-forward svc/budget-service 8085:8085          # Budget
kubectl -n oficina port-forward svc/work-order-service 8086:8086      # Work Order
kubectl -n oficina port-forward svc/notification-service 8087:8087    # Notification
kubectl -n oficina port-forward svc/postgres 5432:5432                # Postgres (para DBeaver)

# 6) Opcional: ajustar StorageClass se sua classe nÃ£o for "standard"
#    (editar k8s/postgres.yaml antes do apply)

# Add-ons Ãºteis (antes do apply, uma vez sÃ³)
minikube addons enable metrics-server
minikube addons enable dashboard
# Se quiser ingress (roteamento HTTP Ãºnico):
minikube addons enable ingress
# Adicione ao /etc/hosts: \"$(minikube ip) oficina.local\"
# Ingress disponÃ­vel em http://oficina.local/<path> (ver caminhos abaixo)
# Abrir dashboard (faz proxy e abre no navegador)
minikube dashboard
```

Add-ons em uso:
- `metrics-server`: coleta mÃ©tricas para o HPA (`k8s/hpa.yaml`).
- `dashboard`: UI web para inspecionar recursos (comando `minikube dashboard`).
- `ingress`: controller ingress-nginx para expor tudo via host `oficina.local`.

Ingress (k8s/ingress.yaml) expÃµe tudo em um host Ãºnico (`oficina.local`) com paths:
- `/eureka` â†’ eureka-server (8761)
- `/auth` â†’ auth-service (8082)
- `/customer` â†’ customer-service (8081)
- `/catalog` â†’ catalog-service (8083)
- `/inventory` â†’ inventory-service (8084)
- `/budget` â†’ budget-service (8085)
- `/work-order` â†’ work-order-service (8086)
- `/notification` â†’ notification-service (8087)

Para limpar tudo (nuke):
```bash
kubectl delete -k k8s || true
minikube stop
minikube delete
```

## ğŸ“Š Monitoramento

### Eureka Dashboard

Acesse http://localhost:8761 para visualizar:
- Todos os serviÃ§os registrados
- Status de cada instÃ¢ncia
- Metadata dos serviÃ§os

### Actuator Endpoints

Todos os serviÃ§os expÃµem endpoints de monitoramento:
- `/actuator/health` - Status de saÃºde do serviÃ§o
- `/actuator/info` - InformaÃ§Ãµes do serviÃ§o
- `/actuator/metrics` - MÃ©tricas da aplicaÃ§Ã£o

## ğŸ”§ Tecnologias

### Backend & Framework
- **Spring Boot 3.5.3** - Framework principal
- **Spring Cloud 2025.0.0** - Cloud native patterns
- **Netflix Eureka** - Service Discovery
- **OpenFeign** - ComunicaÃ§Ã£o entre microserviÃ§os
- **Java 21** - Linguagem de programaÃ§Ã£o
- **Maven** - Gerenciamento de dependÃªncias

### Infraestrutura & Cloud
- **AWS EKS** - Kubernetes gerenciado
- **AWS NLB** - Network Load Balancer (L4)
- **AWS API Gateway V2** - HTTP API para roteamento centralizado
- **AWS VPC Link** - IntegraÃ§Ã£o privada API Gateway â†” VPC
- **Terraform** - Infrastructure as Code (IaC)
- **Kustomize** - Gerenciamento de manifestos Kubernetes
- **Docker** - ContainerizaÃ§Ã£o

### Dados & PersistÃªncia
- **PostgreSQL** - Banco de dados relacional (RDS ou pod StatefulSet)
- **H2** - Banco de dados em memÃ³ria (testes)
- **Flyway** (planejado) - Migrations de banco

### Observabilidade
- **Spring Boot Actuator** - Endpoints de health, metrics, info
- **Datadog** (opcional) - APM e logs centralizados
- **Kubernetes Metrics Server** - MÃ©tricas para HPA

### CI/CD
- **GitHub Actions** (planejado) - Pipelines de build e deploy
- **Docker Hub** - Registry de imagens
- **Makefile** - AutomaÃ§Ã£o de build multi-arch (amd64/arm64)

## ğŸ“š DocumentaÃ§Ã£o da API

### Swagger Agregado

Para facilitar o acesso Ã  documentaÃ§Ã£o de todos os microserviÃ§os em um Ãºnico local, o **Eureka Server** disponibiliza um Swagger agregado:

**ğŸ”— Acesso Ãºnico:** http://localhost:8761/swagger-ui.html (local) ou via API Gateway (cloud)

AtravÃ©s do Swagger agregado, vocÃª pode visualizar e testar as APIs de todos os serviÃ§os atravÃ©s de um dropdown, sem precisar acessar cada serviÃ§o individualmente.

#### Como funciona

- **URLs de documentaÃ§Ã£o**: O Swagger agregado usa `localhost` nas URLs para que o navegador possa acessÃ¡-las
- **ComunicaÃ§Ã£o entre serviÃ§os**: A comunicaÃ§Ã£o interna via Feign continua usando nomes de serviÃ§os (ex: `auth-service`)
- **CORS**: ConfiguraÃ§Ã£o CORS global na `shared-library` permite acesso cross-origin aos endpoints de documentaÃ§Ã£o

### DocumentaÃ§Ã£o Individual dos ServiÃ§os

Cada serviÃ§o tambÃ©m expÃµe sua documentaÃ§Ã£o OpenAPI de forma independente:

- Auth Service: http://localhost:8082/swagger-ui.html
- Customer Service: http://localhost:8081/swagger-ui.html
- Catalog Service: http://localhost:8083/swagger-ui.html
- Inventory Service: http://localhost:8084/swagger-ui.html
- Budget Service: http://localhost:8085/swagger-ui.html
- Work Order Service: http://localhost:8086/swagger-ui.html

### ğŸ“– DocumentaÃ§Ã£o TÃ©cnica Adicional

Para informaÃ§Ãµes detalhadas sobre aspectos especÃ­ficos da arquitetura, consulte:

- **[ROUTING-ARCHITECTURE.md](docs/ROUTING-ARCHITECTURE.md)**: Fluxo completo de roteamento (API Gateway â†’ VPC Link â†’ NLB â†’ Kubernetes), padrÃµes de path, rewrite rules do NGINX Ingress
- **[KUBERNETES-IMAGE-UPDATE-STRATEGY.md](docs/KUBERNETES-IMAGE-UPDATE-STRATEGY.md)**: EstratÃ©gias para atualizaÃ§Ã£o de imagens Docker no Kubernetes
- **[SEED-DATA-AND-API-TESTS.md](docs/SEED-DATA-AND-API-TESTS.md)**: Dados de seed para testes e exemplos de chamadas de API
- **[SWAGGER-AGGREGATION.md](docs/SWAGGER-AGGREGATION.md)**: ImplementaÃ§Ã£o do Swagger agregado no Eureka Server
- **[k8s/README.md](k8s/README.md)**: Guia bÃ¡sico de deploy Kubernetes
- **[k8s/README-OVERLAYS.md](k8s/README-OVERLAYS.md)**: Detalhes sobre Kustomize overlays e diferenÃ§as entre ambientes
- **[AGENTS.md](AGENTS.md)**: Diretrizes para desenvolvimento e contribuiÃ§Ã£o (estrutura, build, testes, commits)

## ğŸ” Troubleshooting

### ServiÃ§os nÃ£o aparecem no Eureka

1. Verifique se o Eureka Server estÃ¡ rodando
2. Aguarde atÃ© 30 segundos - o registro pode levar alguns segundos
3. Verifique os logs do serviÃ§o para erros de conexÃ£o

### Erro de comunicaÃ§Ã£o entre serviÃ§os

1. Certifique-se de que todos os serviÃ§os estÃ£o registrados no Eureka
2. Verifique se a variÃ¡vel de ambiente `EUREKA_CLIENT_SERVICEURL_DEFAULTZONE` estÃ¡ configurada corretamente
3. Confirme que os nomes dos serviÃ§os nos FeignClients correspondem aos nomes registrados no Eureka

### Problemas com Docker

```bash
# Reconstruir imagens (dev)
docker compose --profile dev up -d --build

# Limpar volumes
docker compose --profile dev down -v

# Reiniciar tudo
docker compose --profile dev up -d --force-recreate
```

### Problemas com Terraform

**Erro: "No changes detected" mas recursos nÃ£o foram criados**
```bash
# Verificar se o workspace estÃ¡ correto
terraform workspace list
terraform workspace select dev

# ForÃ§ar re-aplicaÃ§Ã£o
terraform taint kubectl_manifest.kustomization
terraform apply -var-file=environments/dev.tfvars
```

**Erro: "Resource already exists" ou conflitos de ownership**
```bash
# Importar recursos existentes
terraform import kubernetes_namespace.oficina oficina-mecanica-dev

# Ou usar server-side apply (jÃ¡ configurado em k8s.tf)
# O parÃ¢metro force_conflicts=true sobrescreve campos gerenciados por outros controllers
```

**Erro: "Script update_kustomize.sh failed"**
```bash
# Verificar permissÃµes do script
chmod +x infra/scripts/update_kustomize.sh

# Testar manualmente
cd k8s/overlays/dev
kustomize edit set image grecomilani/oficina-eureka-server=grecomilani/oficina-eureka-server:latest
kubectl kustomize .
```

**Erro: VPC Link ou NLB nÃ£o provisiona**
```bash
# Verificar se o EKS cluster existe
aws eks describe-cluster --name eks-fiap-oficina-mecanica --region us-east-2

# Verificar subnets e security groups
terraform console
> data.aws_eks_cluster.oficina.vpc_config[0].subnet_ids
> data.aws_vpc.main.id
```

### Problemas com Kustomize

**Erro: "Resource not found" ao aplicar overlays**
```bash
# Verificar se os recursos base existem
kubectl kustomize k8s/base

# Verificar se o overlay estÃ¡ correto
kubectl kustomize k8s/overlays/dev

# Aplicar base primeiro (debug)
kubectl apply -k k8s/base
```

**Imagens nÃ£o atualizam apÃ³s deploy**
```bash
# Verificar se imagePullPolicy estÃ¡ configurado
kubectl -n oficina-mecanica-dev get deploy auth-service -o jsonpath='{.spec.template.spec.containers[0].imagePullPolicy}'

# Deve retornar "Always" para dev
# ForÃ§ar pull da imagem
kubectl -n oficina-mecanica-dev rollout restart deployment auth-service
```

**HPA nÃ£o escala pods**
```bash
# Verificar se Metrics Server estÃ¡ rodando
kubectl -n kube-system get pods -l k8s-app=metrics-server

# Verificar mÃ©tricas dos pods
kubectl -n oficina-mecanica-dev top pods

# Verificar configuraÃ§Ã£o do HPA
kubectl -n oficina-mecanica-dev get hpa
kubectl -n oficina-mecanica-dev describe hpa auth-service-hpa
```

### Problemas com NLB e API Gateway

**NLB nÃ£o roteia trÃ¡fego para pods**
```bash
# Verificar targets registrados no Target Group
aws elbv2 describe-target-health \
  --target-group-arn $(terraform output -raw target_group_arn)

# Verificar Service no Kubernetes
kubectl -n oficina-mecanica-dev get svc eureka-server-nlb
kubectl -n oficina-mecanica-dev describe svc eureka-server-nlb

# Verificar annotations do NLB
kubectl -n oficina-mecanica-dev get svc eureka-server-nlb -o yaml | grep annotations -A 10
```

**API Gateway retorna 503 ou timeout**
```bash
# Verificar VPC Link status
aws apigatewayv2 get-vpc-link --vpc-link-id $(terraform output -raw vpc_link_id)
# Status deve ser "AVAILABLE"

# Verificar security groups
aws ec2 describe-security-groups --group-ids $(terraform output -raw vpc_link_security_group_id)

# Testar NLB diretamente (bypass API Gateway)
NLB_DNS=$(terraform output -raw nlb_dns_name)
curl http://${NLB_DNS}:8761/actuator/health
```

## ğŸ“ Estrutura do Projeto

```
oficina-microservices/
â”œâ”€â”€ infra/                       # Infraestrutura como CÃ³digo (Terraform)
â”‚   â”œâ”€â”€ providers.tf             # AWS, Kubernetes, Kubectl, Helm providers
â”‚   â”œâ”€â”€ k8s.tf                   # Deploy Kustomize + Metrics Server
â”‚   â”œâ”€â”€ nlb.tf                   # Network Load Balancer por ambiente
â”‚   â”œâ”€â”€ apigateway-vpc-link.tf   # VPC Link para API Gateway
â”‚   â”œâ”€â”€ configmap.tf             # ConfigMaps dinÃ¢micos injetados no K8s
â”‚   â”œâ”€â”€ secrets.tf               # Secrets (JWT, DB, notification)
â”‚   â”œâ”€â”€ namespace.tf             # CriaÃ§Ã£o de namespace por workspace
â”‚   â”œâ”€â”€ datadog.tf               # IntegraÃ§Ã£o com Datadog (opcional)
â”‚   â”œâ”€â”€ environments/            # VariÃ¡veis por ambiente (.tfvars)
â”‚   â”‚   â”œâ”€â”€ dev.tfvars
â”‚   â”‚   â””â”€â”€ prod.tfvars
â”‚   â””â”€â”€ scripts/
â”‚       â””â”€â”€ update_kustomize.sh  # Atualiza tags de imagem + aplica Kustomize
â”‚
â”œâ”€â”€ k8s/                         # Manifests Kubernetes (Kustomize)
â”‚   â”œâ”€â”€ base/                    # Recursos comuns a todos os ambientes
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â”œâ”€â”€ postgres.yaml        # StatefulSet (apenas local)
â”‚   â”‚   â”œâ”€â”€ eureka-server.yaml   # Service Discovery
â”‚   â”‚   â”œâ”€â”€ auth-service.yaml    # AutenticaÃ§Ã£o JWT
â”‚   â”‚   â”œâ”€â”€ customer-service.yaml
â”‚   â”‚   â”œâ”€â”€ catalog-service.yaml
â”‚   â”‚   â”œâ”€â”€ inventory-service.yaml
â”‚   â”‚   â”œâ”€â”€ budget-service.yaml
â”‚   â”‚   â”œâ”€â”€ work-order-service.yaml
â”‚   â”‚   â”œâ”€â”€ notification-service.yaml
â”‚   â”‚   â”œâ”€â”€ configmap-shared.yaml
â”‚   â”‚   â””â”€â”€ hpa.yaml             # Horizontal Pod Autoscaler
â”‚   â”‚
â”‚   â””â”€â”€ overlays/                # ConfiguraÃ§Ãµes especÃ­ficas por ambiente
â”‚       â”œâ”€â”€ local/               # Minikube (namespace: oficina)
â”‚       â”‚   â”œâ”€â”€ kustomization.yaml
â”‚       â”‚   â”œâ”€â”€ namespace.yaml
â”‚       â”‚   â”œâ”€â”€ secret-*.yaml    # Secrets hardcoded para dev local
â”‚       â”‚   â””â”€â”€ ingress.yaml     # Host: oficina.local
â”‚       â”‚
â”‚       â”œâ”€â”€ dev/                 # EKS Dev (namespace: oficina-mecanica-dev)
â”‚       â”‚   â”œâ”€â”€ kustomization.yaml
â”‚       â”‚   â”œâ”€â”€ namespace.yaml
â”‚       â”‚   â””â”€â”€ ingress.yaml     # NLB interno, sem PostgreSQL
â”‚       â”‚
â”‚       â””â”€â”€ prod/                # EKS Prod (namespace: oficina-mecanica-prod)
â”‚           â”œâ”€â”€ kustomization.yaml
â”‚           â”œâ”€â”€ namespace.yaml
â”‚           â”œâ”€â”€ ingress.yaml
â”‚           â””â”€â”€ patches/         # Patches para prod (rÃ©plicas, recursos)
â”‚
â”œâ”€â”€ shared-library/              # CÃ³digo compartilhado (DTOs, security, mappers)
â”‚   â””â”€â”€ src/main/java/br/com/fiap/oficina/shared/
â”‚       â”œâ”€â”€ dto/                 # Data Transfer Objects
â”‚       â”œâ”€â”€ validator/           # ValidaÃ§Ãµes customizadas
â”‚       â”œâ”€â”€ security/            # JWT, CORS, configs de seguranÃ§a
â”‚       â”œâ”€â”€ exception/           # Exception handlers globais
â”‚       â”œâ”€â”€ mapper/              # MapStruct mappers
â”‚       â””â”€â”€ constants/           # Constantes e enums
â”‚
â”œâ”€â”€ eureka-server/               # Service Discovery + Swagger agregado
â”œâ”€â”€ auth-service/                # AutenticaÃ§Ã£o JWT + usuÃ¡rios
â”œâ”€â”€ customer-service/            # Clientes e veÃ­culos
â”œâ”€â”€ catalog-service/             # CatÃ¡logo de produtos/serviÃ§os
â”œâ”€â”€ inventory-service/           # Controle de estoque
â”œâ”€â”€ budget-service/              # OrÃ§amentos
â”œâ”€â”€ work-order-service/          # Ordens de serviÃ§o
â”œâ”€â”€ notification-service/        # NotificaÃ§Ãµes assÃ­ncronas (email)
â”‚
â”œâ”€â”€ docs/                        # DocumentaÃ§Ã£o tÃ©cnica
â”‚   â”œâ”€â”€ ROUTING-ARCHITECTURE.md  # Detalhes do roteamento (API Gateway + NLB)
â”‚   â”œâ”€â”€ KUBERNETES-IMAGE-UPDATE-STRATEGY.md
â”‚   â”œâ”€â”€ SEED-DATA-AND-API-TESTS.md
â”‚   â””â”€â”€ SWAGGER-AGGREGATION.md
â”‚
â”œâ”€â”€ api-docs/                    # OpenAPI specs (JSON) baixados dos serviÃ§os
â”œâ”€â”€ docker-compose.yml           # Dev local (perfil: dev)
â”œâ”€â”€ docker-compose.prod.yml      # Prod simulado (perfil: prod)
â”œâ”€â”€ Makefile                     # Comandos para build/push de imagens
â””â”€â”€ pom.xml                      # POM raiz (aggregator)
```

### Conceitos Importantes

#### Shared Library
MÃ³dulo Maven compartilhado que contÃ©m cÃ³digo reutilizÃ¡vel entre todos os microserviÃ§os:
- **DTOs**: Objetos de transferÃªncia de dados padronizados
- **Security**: JWT token utils, CORS config, authentication filters
- **Validators**: ValidaÃ§Ãµes customizadas (CPF, CNPJ, email)
- **Exceptions**: Handlers globais para respostas de erro consistentes
- **Mappers**: MapStruct para conversÃ£o entre entidades e DTOs
- **Constants**: Enums, constantes, mensagens de erro

Todos os microserviÃ§os dependem dessa biblioteca via Maven dependency.

#### Kustomize Base + Overlays
- **base/**: Define recursos Kubernetes comuns (Deployments, Services, ConfigMaps, HPA)
- **overlays/**: Customiza a base para cada ambiente via patches (rÃ©plicas, recursos, secrets, ingress)
- Permite reutilizaÃ§Ã£o de cÃ³digo e separaÃ§Ã£o clara de ambientes sem duplicaÃ§Ã£o

#### Terraform Workspaces
- **dev**: Ambiente de desenvolvimento (NLB interno, 1 rÃ©plica, secrets simples)
- **prod**: Ambiente de produÃ§Ã£o (NLB pÃºblico, 2+ rÃ©plicas, secrets gerenciados)
- Cada workspace mantÃ©m estado isolado no S3 backend

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### ğŸ“‹ Diretrizes de ContribuiÃ§Ã£o

Antes de contribuir, leia [AGENTS.md](AGENTS.md) para entender:
- Estrutura de pacotes e mÃ³dulos
- ConvenÃ§Ãµes de cÃ³digo e nomenclatura
- Como executar testes (`mvn test`, `mvn verify -Pverify-coverage`)
- Requisitos de cobertura de cÃ³digo (â‰¥50% PR, â‰¥70% main)
- PadrÃ£o de commits e PRs

## ğŸš€ Roadmap e Melhorias Futuras

### CI/CD
- [ ] GitHub Actions para build automÃ¡tico
- [ ] Pipeline de deploy automatizado (dev â†’ staging â†’ prod)
- [ ] Testes de integraÃ§Ã£o automatizados
- [ ] ValidaÃ§Ã£o de cobertura de cÃ³digo no PR

### Infraestrutura
- [ ] AWS Secrets Manager para prod (substituir Kubernetes Secrets)
- [ ] RDS PostgreSQL provisionado via Terraform
- [ ] Route53 + Certificate Manager para HTTPS
- [ ] CloudWatch Logs integration
- [ ] Backup automatizado de banco de dados

### Observabilidade
- [ ] Datadog APM completo
- [ ] Distributed tracing (Sleuth + Zipkin ou OpenTelemetry)
- [ ] Dashboards Grafana + Prometheus
- [ ] Alertas proativos (SLA violations, latÃªncia, erros)

### SeguranÃ§a
- [ ] OAuth2 / OpenID Connect integration
- [ ] Rate limiting no API Gateway
- [ ] WAF (Web Application Firewall)
- [ ] Secrets rotation automatizado
- [ ] Vulnerability scanning (Snyk, Trivy)

### Features de AplicaÃ§Ã£o
- [ ] Webhooks para notificaÃ§Ãµes
- [ ] IntegraÃ§Ã£o com sistemas de pagamento
- [ ] RelatÃ³rios e analytics
- [ ] Mobile app integration
- [ ] Sistema de chat/suporte

---

## ğŸ“ Suporte

Para dÃºvidas ou problemas:
1. Verifique a seÃ§Ã£o [Troubleshooting](#-troubleshooting)
2. Consulte a [documentaÃ§Ã£o tÃ©cnica](docs/)
3. Abra uma issue no GitHub
4. Entre em contato com a equipe de desenvolvimento

---

**Desenvolvido com â¤ï¸ pela equipe FIAP - Oficina Microservices**
